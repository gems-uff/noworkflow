{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Summer of Reproducibility - noWorkflow Base Experiment - Notebook 4\n",
    "\n",
    "This Jupyter Notebook is dedicated to guiding you through the applications of noWorkflow in Data Science and Machine Learning. It is the outcome of our participation in the Summer of Reproducibility at OSPO UCSC 2023, utilizing [noWorkflow](https://github.com/gems-uff/noworkflow).\n",
    "\n",
    "This Notebook serves as a use case based on the problem of Fraud Detection. We have partially replicated the work entitled \"The Effect of Feature Extraction and Data Sampling on Credit Card Fraud Detection.\" Interested readers are encouraged to refer to the original paper [here](https://link.springer.com/article/10.1186/s40537-023-00684-w).\n",
    "\n",
    "For the sake of clarity, we have divided this experiment into different notebooks:\n",
    "\n",
    "1. Covers the steps from reading the dataset to a Random Forest model training, configuring a single trial.\n",
    "2. Repeats all previous steps but with changes in the experimental setup, such as modified hyperparameters.\n",
    "3. Utilizes noWorkflow to summarize the results from previous trials.\n",
    "4. Repeat the experiment, changing the model and the order of operations.\n",
    "5. Compares the modifications and differences between the last and first experiments.\n",
    "\n",
    "**Please, remember to select the noWorkflow kernel before running these Notebooks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from noworkflow.now.tagging.var_tagging import backward_deps, \\\n",
    "    global_backward_deps, store_operations, resume_trials, trial_diff, \\\n",
    "    trial_intersection_diff, var_tag_plot, var_tag_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/creditcard.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering stage\n",
    "\n",
    "Separate the features and target variables. The first step in feature treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering: applying random undersampling to the extracted features\n",
    "\n",
    "In this experiment, we are inverting the sequence between RandomUnderSampler and PCA calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation(id=33, checkpoint=23.406392426, code_component_id=710, activation_id=30, repr=42)\n"
     ]
    }
   ],
   "source": [
    "random_seed = now_variable('random_seed', 42)\n",
    "rus = RandomUnderSampler(random_state=random_seed)\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering: Apply PCA for feature extraction.\n",
    "\n",
    "Here, we define the *pca_components* tag to keep the n_components argument in PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation(id=53, checkpoint=23.658996901, code_component_id=748, activation_id=50, repr=3)\n"
     ]
    }
   ],
   "source": [
    "pca_components = now_variable('pca_components', 3)\n",
    "pca = PCA(n_components=pca_components)  # Adjust the number of components as needed\n",
    "X_pca = pca.fit_transform(X_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Feature engineering: splitting the dataset into trains and tests\n",
    "\n",
    "Here we have two hyperparameter assignments: the proportion of the test_size and the random_state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation(id=70, checkpoint=23.799512257, code_component_id=779, activation_id=67, repr=0.2)\n"
     ]
    }
   ],
   "source": [
    "test_dim = now_variable('test_dim', 0.2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y_resampled, test_size=test_dim, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring: model training and transforming features into predictions\n",
    "##### XGBoost\n",
    "\n",
    "Instantiate and evaluate an XGBoost classifier. Here we are tagging the model name with a model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation(id=90, checkpoint=23.897578456999998, code_component_id=813, activation_id=85, repr=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=None,\n",
      "              reg_alpha=None, reg_lambda=None, ...))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = now_variable('model', xgb.XGBClassifier())\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluating: evaluating the performance of models\n",
    "##### RandomForest\n",
    "\n",
    "Computing performance metrics. Two control variables are tagged here. *roc_rf* stores the ROC score as a classical metric in classification. On the other hand, *f1_rf* is the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation(id=111, checkpoint=24.08765099, code_component_id=851, activation_id=100, repr=0.8780663780663781)\n",
      "Evaluation(id=120, checkpoint=24.090723284, code_component_id=867, activation_id=100, repr=0.875)\n",
      "XGBoost - ROC = 0.878066, F1 = 0.875000\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "roc_metric = now_variable('roc_metric', roc_auc_score(y_test, y_pred))\n",
    "f1_metric = now_variable('f1_metric', f1_score(y_test, y_pred))\n",
    "\n",
    "print(\"XGBoost - ROC = %f, F1 = %f\" % (roc_metric, f1_metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiment dependencies from roc_metric variable\n",
    "\n",
    "When calling the backward_deps('tagged_var_name'), \n",
    "we receive a list of variables that are involved in the computation of the tagged variable. In this example, if you call it with the 'roc_metric' tag, the output will include all operations that were involved in the construction of its final value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{25: ('y_test', 'complex data type'),\n",
       " 24: (\"now_variable('model', xgb.XGBClassifier())\", 'complex data type'),\n",
       " 23: ('xgb_model', 'complex data type'),\n",
       " 22: (\"now_variable('pca_components', 3)\", '3'),\n",
       " 21: ('pca_components', '3'),\n",
       " 20: ('PCA(n_components=pca_components)', 'PCA(n_components=3)'),\n",
       " 19: ('pca', 'PCA(n_components=3)'),\n",
       " 18: ('X_resampled', 'complex data type'),\n",
       " 17: ('X_pca', 'complex data type'),\n",
       " 16: ('RandomUnderSampler(random_state=random_seed)', 'complex data type'),\n",
       " 15: ('rus', 'complex data type'),\n",
       " 14: ('X', 'complex data type'),\n",
       " 13: ('df', 'complex data type'),\n",
       " 12: (\"df['Class']\", 'complex data type'),\n",
       " 11: ('y', 'complex data type'),\n",
       " 10: ('y_resampled', 'complex data type'),\n",
       " 9: (\"now_variable('test_dim', 0.2)\", '0.2'),\n",
       " 8: ('test_dim', '0.2'),\n",
       " 7: (\"now_variable('random_seed', 42)\", '42'),\n",
       " 6: ('random_seed', '42'),\n",
       " 5: ('train_test_split(X_pca, y_resampled, test_size=test_dim, random_state=random_seed)',\n",
       "  'complex data type'),\n",
       " 4: ('X_test', 'complex data type'),\n",
       " 3: ('y_pred', 'complex data type'),\n",
       " 2: ('roc_auc_score(y_test, y_pred)', '0.8780663780663781'),\n",
       " 1: (\"now_variable('roc_metric', roc_auc_score(y_test, y_pred))\",\n",
       "  '0.8780663780663781'),\n",
       " 0: ('roc_metric', '0.8780663780663781')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_ops = backward_deps('roc_metric', False)\n",
    "dict_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiment dependencies from roc_metric\n",
    "Save the operations dictionary in a shelve object with this trial_id as a key.\n",
    "\n",
    "Steps are:\n",
    "1. calls store operations() to store the dict into a shelve object with this trial_id key.\n",
    "2. Verify the list of stored trials available for comparison with resume_trials()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary stored in shelve.\n"
     ]
    }
   ],
   "source": [
    "trial_id = __noworkflow__.trial_id\n",
    "store_operations(trial_id, dict_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['edb94455-f97b-46f0-b30e-ed01eaf81081',\n",
       " 'b86773c3-a3b7-40d0-a3ac-5ab4278826c2',\n",
       " 'c33177a6-88be-4f78-ae96-ede68a5ab142']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_trials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Next steps\n",
    "The final [Notebook](./now_usecase_part_5.ipynb) will use the noWorkflow features to compare this experiment and the first one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noWorkflow 3",
   "language": "python",
   "name": "noworkflow3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd46906d0be51105938edee03e9704979453c4958d5b4d09c310e6ecda521c36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
